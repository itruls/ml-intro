{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting into the basics of machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a Python-based introduction to machine learning, covering only the basics, and intended for a half day session. \n",
    "\n",
    "Some reasons for using Python is given at https://www.quora.com/Why-is-Python-a-language-of-choice-for-data-scientists. Also, in may 2017, Python overtook R as most popular among KDnuggets 2900 voters in it's [annual poll](http://www.kdnuggets.com/2017/05/poll-analytics-data-science-machine-learning-software-leaders.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "Why is machine learning \"suddenly\" so popular? I think a lot of the interest is due to the successes of deep learning the last few years. And since deep learning is a type of machine learning, many want to start with the basics.\n",
    "\n",
    "![Nice illustration of relationship between AI, ML and DL from NVidia](https://blogs.nvidia.com/wp-content/uploads/2016/07/Deep_Learning_Icons_R5_PNG.jpg.png)\n",
    "(image from NVidia's blog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Data Science\n",
    "A nice illustration from Drew Conways blog gives some insight:\n",
    "![Data Science Venn diagram from Conway](https://static1.squarespace.com/static/5150aec6e4b0e340ec52710a/t/51525c33e4b0b3e0d10f77ab/1364352052403/Data_Science_VD.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is machine learning?\n",
    "\n",
    "The process of iteratively learning a model from data. This model might be used for prediction of previously unseen data (supervised learning). So by providing the algorithm with lots of observations from a domain, it infers the model automatically.\n",
    "\n",
    "One example is the classification of email into the classes spam & no-spam. The observation here is the email and metadata about it.\n",
    "\n",
    "A more detailed explanation is found at\n",
    "https://github.com/amueller/scipy_2015_sklearn_tutorial/blob/master/notebooks/01.1%20Introduction%20to%20Machine%20Learning.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory\n",
    "\n",
    "3 types of learning problems (by learning style):\n",
    "1. **Supervised learning**: Given input X and output y find f such that f(X) = y\n",
    "    1. classification (discrete y, a category): E.g. spam/no-spam\n",
    "    2. regression (continuous y, a real value): E.g. house price\n",
    "2. **Unsupervised learning**\n",
    "    1. clustering\n",
    "    2. dimensionality reduction\n",
    "    3. density estimation\n",
    "    4. association: E.g. people that buy A also tend to buy B\n",
    "    5. ...\n",
    "3. **Reinforcement learning** (learning from trial-and-error by rewards/punishment)\n",
    "\n",
    "This tutorial will be on supervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools / setup\n",
    "- [Anaconda](https://docs.continuum.io/anaconda/) with Python 3.6: Data science platform for installing all needed tools.\n",
    "- Python 3.5/6 ([Python for Data Science pdf cheat sheet from DataCamp](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PythonForDataScience.pdf))\n",
    "- [Jupyter Notebook](http://jupyter.org): Web-based interactive computational environment combines code execution, rich\n",
    "text, mathematics, plots and rich media.\n",
    "\n",
    "Useful Python Data Science packages included with Anaconda:\n",
    "- [NumPy](http://www.numpy.org/) for Scientific Computing ([Numpy pdf cheat sheet from DataCamp](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Numpy_Python_Cheat_Sheet.pdf))\n",
    "- [Pandas](http://pandas.pydata.org/) for Data Structure & Analysis ([Pandas cheat sheet](https://github.com/pandas-dev/pandas/blob/master/doc/cheatsheet/Pandas_Cheat_Sheet.pdf))\n",
    "- [Matplotlib](http://matplotlib.org/contents.html) for plotting and vizualizing data.\n",
    "- [Seaborn](http://stanford.edu/~mwaskom/software/seaborn/) for Statistical data visualization (based on matplotlib)\n",
    "- [SciKit-learn](http://scikit-learn.org/stable/documentation.html) for machine learning and data mining\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out https://github.com/rasbt/watermark for making reproducible analysis easier.\n",
    "%watermark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing useful libraries\n",
    "(and verifying our installation worked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Support for type hints\n",
    "import typing\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd  # Data Analysis Library: high-performance, easy-to-use data structures and data analysis tools\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb  # Statistical data visualization\n",
    "# Display plots inside notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Machine learning sample datasets and algorithms\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised learning example: Linear regression with linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Based on https://github.com/jakevdp/PMLC-2014/blob/master/fig_code/linear_regression.py\n",
    "max_x = 30\n",
    "np.random.seed(0)\n",
    "\n",
    "def generate_linear_data(n: int) -> (np.array, np.array):\n",
    "    a = 0.5\n",
    "    b = 1.0\n",
    "\n",
    "    # generate n random values in the range [0, max_x)\n",
    "    x = np.sort(max_x * np.random.random(n))\n",
    "\n",
    "    # y = a*x + b with noise\n",
    "    noise = np.random.normal(size=x.shape)\n",
    "    y = a * x + b + noise\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "    \n",
    "def create_linear_model(x: np.array, y: np.array) -> (np.array, np.array):\n",
    "    # create a linear regression model\n",
    "    ## linear regression attempts to draw a straight line that will \n",
    "    ## best minimize the residual sum of squares between the observed \n",
    "    ## responses in the dataset\n",
    "    model = LinearRegression()\n",
    "    # Setting correct dimension\n",
    "    X = x.reshape(-1,1)\n",
    "    model.fit(X, y)\n",
    "    print(\"Model error on samples: {:.2f}\".format(model.score(X,y)))\n",
    "    # *-operator is unpacking argument list (https://docs.python.org/3/tutorial/controlflow.html#unpacking-argument-lists)\n",
    "    print(\"Model parameters: a = {:.2f}, b = {:.2f}\".format(*model.coef_, model.intercept_))\n",
    "    return model\n",
    "\n",
    "\n",
    "def plot_results(x_samples: np.array, y_samples: np.array,\n",
    "                 model):\n",
    "    # 100 values with equal distance from 0 to max_x\n",
    "    x_pred = np.linspace(0, max_x, 100)\n",
    "    # predict y from the data\n",
    "    y_pred = model.predict(x_pred.reshape(-1,1))\n",
    "    \n",
    "    ax = plt.axes()\n",
    "    # Scatter plot samples\n",
    "    ax.scatter(x_samples, y_samples, label='Samples')\n",
    "    # Plot prediction model\n",
    "    ax.plot(x_pred, y_pred, label='Model')\n",
    "    \n",
    "    # Plot error lines\n",
    "    label = False\n",
    "    for x, y in zip(x_samples, y_samples):\n",
    "        y_model = model.predict(x)\n",
    "        # plot vertical error line from model to sample\n",
    "        # 2 x,y pairs: (x_sample, y_model) and (x_sample, y_sample)\n",
    "        if not label:\n",
    "            ax.plot( [x,x], [y_model,y], 'r', label='Error' )\n",
    "            label = True\n",
    "        else:\n",
    "            ax.plot( [x,x], [y_model,y], 'r')\n",
    "    \n",
    "    # Plotting specifics\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.legend(loc='best')\n",
    "    ax.axis('tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with larger values than 20, e.g. 1000, and see that model parameters are closer to original line: a=0.5, b=1.0\n",
    "x, y = generate_linear_data(20)\n",
    "model = create_linear_model(x, y)\n",
    "plot_results(x, y, model)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Line is fitted to samples/points by minimizing the total error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of linear regression with higher order polynomial + overfitting/underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "n_samples = 30\n",
    "degrees = [1, 4, 15]\n",
    "\n",
    "true_fun = lambda X: np.cos(1.5 * np.pi * X)\n",
    "X = np.sort(np.random.rand(n_samples))\n",
    "y = true_fun(X) + np.random.randn(n_samples) * 0.1\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "for i in range(len(degrees)):\n",
    "    ax = plt.subplot(1, len(degrees), i + 1)\n",
    "    plt.setp(ax, xticks=(), yticks=())\n",
    "\n",
    "    polynomial_features = PolynomialFeatures(degree=degrees[i],\n",
    "                                             include_bias=False)\n",
    "    linear_regression = LinearRegression()\n",
    "    pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                         (\"linear_regression\", linear_regression)])\n",
    "    pipeline.fit(X[:, np.newaxis], y)\n",
    "\n",
    "    # Evaluate the models using crossvalidation\n",
    "    scores = cross_val_score(pipeline, X[:, np.newaxis], y,\n",
    "                             scoring=\"neg_mean_squared_error\", cv=10)\n",
    "\n",
    "    X_test = np.linspace(0, 1, 100)\n",
    "    plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label=\"Model\")\n",
    "    plt.plot(X_test, true_fun(X_test), label=\"True function\")\n",
    "    plt.scatter(X, y, label=\"Samples\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.xlim((0, 1))\n",
    "    plt.ylim((-2, 2))\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title(\"Degree {}\\nMSE = {:.2e}(+/- {:.2e})\".format(\n",
    "        degrees[i], -scores.mean(), scores.std()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of Classification with K-Nearest Neighbor Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Create color maps for 3-class classification problem, as with iris\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "def plot_iris_knn():\n",
    "    iris = load_iris()\n",
    "    X = iris.data[:, :2]  # we only take the first two features. We could\n",
    "                        # avoid this ugly slicing by using a two-dim dataset\n",
    "    y = iris.target\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=3)\n",
    "    knn.fit(X, y)\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - .1, X[:, 0].max() + .1\n",
    "    y_min, y_max = X[:, 1].min() - .1, X[:, 1].max() + .1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                         np.linspace(y_min, y_max, 100))\n",
    "    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure()\n",
    "    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)\n",
    "    plt.xlabel('sepal length (cm)')\n",
    "    plt.ylabel('sepal width (cm)')\n",
    "    plt.title(\"Score = {:.2f}(+/- {:.2e})\".format(knn.score(X,y), knn.score(X,y)))\n",
    "    plt.axis('tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_iris_knn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow (based on [CRISP-DM](https://en.wikipedia.org/wiki/Cross_Industry_Standard_Process_for_Data_Mining))\n",
    "![CRISP-DM diagram](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b9/CRISP-DM_Process_Diagram.png/479px-CRISP-DM_Process_Diagram.png)\n",
    "(By Kenneth Jensen [CC BY-SA 3.0 (http://creativecommons.org/licenses/by-sa/3.0)], via Wikimedia Commons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Business/Problem understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is the problem?\n",
    "  - Informal description\n",
    "  - Formal description\n",
    "2. Why does the problem need to be solved?\n",
    "3. How would I solve the problem manually?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Collect initial data\n",
    "Which data sources do we have available?\n",
    "- Internal\n",
    "- Partners\n",
    "- External\n",
    "\n",
    "What format?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Describe data\n",
    "    - What are the features?\n",
    "    - How many samples?\n",
    "3. Explore data\n",
    "4. Verify data quality\n",
    "    - Missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following Classification-example with the Iris flowers is based on the excellent (and much more detailed) notebook https://github.com/rhiever/Data-Analysis-and-Machine-Learning-Projects/blob/master/example-data-science-notebook/Example%20Machine%20Learning%20Notebook.ipynb.\n",
    "\n",
    "A similar tutorial is also given at [machinelearningmastery.com](http://machinelearningmastery.com/machine-learning-in-python-step-by-step/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "iris_dataset = load_iris()\n",
    "# Describe (provided description in dataset)\n",
    "print(iris_dataset.DESCR)\n",
    "# TODO: Picture of the different flowers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data into pandas Dataframe\n",
    "iris_data = pd.DataFrame(iris_dataset.data, columns=iris_dataset.feature_names)\n",
    "print(iris_dataset.target_names)\n",
    "# Convert 0,1,2 in target column to iris_dataset.target_names\n",
    "iris_data['class'] = iris_dataset.target\n",
    "iris_data['class'] = iris_data['class'].apply(lambda i: iris_dataset.target_names[i])\n",
    "iris_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics about dataset\n",
    "iris_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for each class\n",
    "iris_data.groupby('class').describe(percentiles=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: All features are numbers. If not, we have to convert them by e.g. One-hot-encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many instances in each class of flowers?\n",
    "iris_data.groupby('class').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x-axis is flower number, 0-49 is one class of flowers, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot pairwise relationships in dataset\n",
    "# Note: Only possible to print in 2D (3D), so thats a visualization restriction even when we work on higher dimensional data.\n",
    "sb.pairplot(iris_data, hue='class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diagonal displays distribution of each feature, off-diagonal displays pairwise correlation between the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setosa has an outlier on sepal width\n",
    "print(iris_data.columns.values)\n",
    "# Filter all rows that have class=='setosa', and select only sepal width column\n",
    "iris_data[iris_data['class'] == 'setosa']['sepal width (cm)'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "# Making Boxplots of each of the features, split on class\n",
    "for column_index, column in enumerate(iris_data.columns):\n",
    "    if column == 'class':\n",
    "        continue\n",
    "    plt.subplot(2, 2, column_index + 1)    \n",
    "    # Boxplot\n",
    "    sb.boxplot(x='class', y=column, data=iris_data)\n",
    "    # Violinplot is like boxplot but box is scaled according to density of the data\n",
    "    #sb.violinplot(x='class', y=column, data=iris_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation\n",
    "Really important (and time-consuming), but not prioritized in this short introduction.\n",
    "\n",
    "1. Select data\n",
    "2. Clean data\n",
    "    - Drop samples with missing features?\n",
    "3. Construct data\n",
    "    - Impute missing data?\n",
    "4. Integrate data\n",
    "5. Format data\n",
    "6. Dataset\n",
    "\n",
    "**Bad data => Bad model** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling\n",
    "\n",
    "1. Select modeling technique\n",
    "2. Generate test design\n",
    "3. Build model\n",
    "4. Assess model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree learning\n",
    "a Greedy learning algorithm:\n",
    "\n",
    "- Repeat until no or small improvement in the purity\n",
    "    1. Find the attribute with the highest gain\n",
    "    2. Add the attribute to the tree and split the set accordingly\n",
    "- Builds the tree in the top-down fashion\n",
    "    - Gradually expands the leaves of the partially built tree\n",
    "- The method is greedy\n",
    "    - It looks at a single attribute and gain in each step\n",
    "    - May fail when the combination of attributes is needed to improve the purity (parity functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gini impurity is a measure of misclassification.\n",
    "\n",
    "Impurity measure defines how well the classes are separated.\n",
    "\n",
    "In general the impurity measure should satisfy:\n",
    "- Largest when data are split evenly for attribute values\n",
    "- Should be 0 when all data belong to the same class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. Using Decision Tree as modeling technique:\n",
    "decision_tree_classifier = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Generate test design\n",
    "# Load data in input (X) and output (y) variables\n",
    "y = iris_data['class'].values\n",
    "X = iris_data.drop('class', axis=1).values\n",
    "\n",
    "print(\"Shape of input: %s\" % str(X.shape))\n",
    "print(\"Shape of output: %s\" % str(y.shape))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split data into (75%) training and (25%) test set. \n",
    "# Fixing random state for demonstration purpose\n",
    "(X_train, X_test, y_train, y_test) = train_test_split(X, y, train_size=0.75, random_state=1)\n",
    "print(\"Shape of training data: %s\" % str(X_train.shape))\n",
    "print(\"Shape of test data: %s\" % str(X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Build model\n",
    "# Train the classifier on the training set\n",
    "decision_tree_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the decision tree\n",
    "from IPython.display import Image  \n",
    "from sklearn import tree\n",
    "import pydotplus\n",
    "\n",
    "def visualize_decision_tree(decision_tree_classifier):\n",
    "    dot_data = tree.export_graphviz(decision_tree_classifier, out_file=None, \n",
    "                             feature_names=iris_dataset.feature_names,  \n",
    "                             class_names=iris_dataset.target_names,  \n",
    "                             filled=True, rounded=True,  \n",
    "                             special_characters=True)  \n",
    "\n",
    "    graph = pydotplus.graph_from_dot_data(dot_data)  \n",
    "    return Image(graph.create_png())\n",
    "\n",
    "visualize_decision_tree(decision_tree_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to measure our success?\n",
    "TODO: Different ways of measuring. What is a good way? Depends on problem.\n",
    "https://en.wikipedia.org/wiki/Confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict class on whole test set\n",
    "y_pred = decision_tree_classifier.predict(X_test)\n",
    "\n",
    "# Compare prediction of first element in test set with real class\n",
    "print(\"Predicted class first element: {}\".format(y_pred[0]))\n",
    "# Real class:\n",
    "print(\"Real class first element: {}\".format(y_test[0]))\n",
    "#print(y_pred)\n",
    "#print(y_test)\n",
    "correct_predictions = np.sum(y_pred == y_test)\n",
    "print(\"Count number of predictions equal to real class: {:d}\".format(correct_predictions))\n",
    "total_predictions = y_pred.shape[0]\n",
    "print(\"Total number of predictions: {:d}\".format(total_predictions))\n",
    "accuracy = correct_predictions / total_predictions\n",
    "print(\"Accuracy {}/{}={:2f}\".format(correct_predictions, total_predictions, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the classifier on the testing set using classification accuracy\n",
    "decision_tree_classifier.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is that good? Depends on the sampling in train/test-split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how accuracy varies with different samples (random_state is not fixed)\n",
    "model_accuracies = []\n",
    "\n",
    "for repetition in range(1000):\n",
    "    (X_train, X_test, y_train, y_test) = train_test_split(X, y, train_size=0.75)\n",
    "    \n",
    "    decision_tree_classifier = DecisionTreeClassifier()\n",
    "    decision_tree_classifier.fit(X_train, y_train)\n",
    "    classifier_accuracy = decision_tree_classifier.score(X_test, y_test)\n",
    "    model_accuracies.append(classifier_accuracy)\n",
    "    \n",
    "ax = sb.distplot(model_accuracies)\n",
    "ax.set_title('Average score: {:.2f} ({:.3f})'.format(np.mean(model_accuracies), np.std(model_accuracies)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Big variations in how our model perform for different subsets of training data.\n",
    "\n",
    "Does not generalize well on new data (X_test) => **overfitting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem is the main reason that most data scientists perform **k-fold cross-validation** on their models: Split the original data set into k subsets, use one of the subsets as the testing set, and the rest of the subsets are used as the training set. This process is then repeated k times such that each subset is used as the testing set exactly once.\n",
    "10-fold cross-validation is the most common choice, so let's use that here. Performing 10-fold cross-validation on our data set looks something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def plot_cv(cv, n_samples):\n",
    "    masks = []\n",
    "    for train, test in cv:\n",
    "        mask = np.zeros(n_samples, dtype=bool)\n",
    "        mask[test] = 1\n",
    "        masks.append(mask)\n",
    "        \n",
    "    plt.figure(figsize=(15, 15))\n",
    "    plt.imshow(masks, interpolation='none')\n",
    "    plt.ylabel('Fold')\n",
    "    plt.xlabel('Row #')\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "plot_cv(skf.split(X,y), len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stratified k-fold cross-validation**: Stratified means the class proportions the same across all of the folds, so we maintain a representative subset of our data set. (e.g., so we don't have 100% Iris setosa entries in one of the folds.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "decision_tree_classifier = DecisionTreeClassifier()\n",
    "\n",
    "# cross_val_score returns a list of the scores, which we can visualize\n",
    "# to get a reasonable estimate of our classifier's performance\n",
    "cv_scores = cross_val_score(decision_tree_classifier, X, y, cv=10)\n",
    "sb.distplot(cv_scores)\n",
    "plt.title('Average score: {:.2f} ({:.3f})'.format(np.mean(cv_scores), np.std(cv_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning\n",
    "\n",
    "Limiting the depth of decision tree classifier gives much lower accuracy. What hyperparameter choice is best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# max_depth=1 gives low score\n",
    "decision_tree_classifier = DecisionTreeClassifier(max_depth=1)\n",
    "\n",
    "cv_scores = cross_val_score(decision_tree_classifier, X, y, cv=10)\n",
    "sb.distplot(cv_scores, kde=False)\n",
    "plt.title('Average score: {:.2f}'.format(np.mean(cv_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One common systematic way to find best parameters for model and data set: **Grid Search**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "decision_tree_classifier = DecisionTreeClassifier()\n",
    "\n",
    "# Which parameters can be searched\n",
    "print(decision_tree_classifier.get_params())\n",
    "\n",
    "parameter_grid = {'max_depth': [1, 2, 3, 4, 5],\n",
    "                  'max_features': [1, 2, 3, 4]}\n",
    "\n",
    "grid_search = GridSearchCV(decision_tree_classifier,\n",
    "                           param_grid=parameter_grid,\n",
    "                           cv=10)\n",
    "\n",
    "grid_search.fit(X, y)\n",
    "print('Best score: {:.2f}'.format(grid_search.best_score_))\n",
    "print('Best parameters: {}'.format(grid_search.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid_visualization = grid_search.cv_results_['mean_test_score']\n",
    "\n",
    "depth_count = len(parameter_grid['max_depth']) # 5\n",
    "feature_count = len(parameter_grid['max_features']) # 4\n",
    "\n",
    "grid_visualization = np.array(grid_visualization)\n",
    "grid_visualization.shape = (depth_count, feature_count)\n",
    "sb.heatmap(grid_visualization, cmap='Blues')\n",
    "plt.xticks(np.arange(feature_count) + 0.5, grid_search.param_grid['max_features'])\n",
    "plt.yticks(np.arange(depth_count) + 0.5, grid_search.param_grid['max_depth'][::-1])\n",
    "plt.xlabel('max_features')\n",
    "plt.ylabel('max_depth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "decision_tree_classifier = DecisionTreeClassifier()\n",
    "\n",
    "parameter_grid = {'criterion': ['gini', 'entropy'],\n",
    "                  'splitter': ['best', 'random'],\n",
    "                  'max_depth': [1, 2, 3, 4, 5],\n",
    "                  'max_features': [1, 2, 3, 4]}\n",
    "\n",
    "grid_search = GridSearchCV(decision_tree_classifier,\n",
    "                           param_grid=parameter_grid,\n",
    "                           cv=10)\n",
    "\n",
    "grid_search.fit(X, y)\n",
    "print('Best score: {:.2f}'.format(grid_search.best_score_))\n",
    "print('Best parameters: {}'.format(grid_search.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_decision_tree_classifier = grid_search.best_estimator_\n",
    "best_decision_tree_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_decision_tree(best_decision_tree_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "random_forest_classifier = RandomForestClassifier()\n",
    "\n",
    "parameter_grid = {'n_estimators': [5, 10, 25, 50],\n",
    "                  'criterion': ['gini', 'entropy'],\n",
    "                  'max_features': [1, 2, 3, 4],\n",
    "                  'warm_start': [True, False]}\n",
    "\n",
    "grid_search = GridSearchCV(random_forest_classifier,\n",
    "                           param_grid=parameter_grid,\n",
    "                           cv=10)\n",
    "\n",
    "grid_search.fit(X, y)\n",
    "print('Best score: {}'.format(grid_search.best_score_))\n",
    "print('Best parameters: {}'.format(grid_search.best_params_))\n",
    "\n",
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation\n",
    "\n",
    "1. Evaluate results\n",
    "2. Review process\n",
    "    - Reproducibility\n",
    "3. Determine next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Deployment (Not covered)\n",
    "- Monitoring (Dashboards, Reports)\n",
    "- Model Management: When to update a model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- [machinelearningmastery.com](http://machinelearningmastery.com/): An excellent blog by Jason Brownlee\n",
    "- [KDnuggets.com](http://www.kdnuggets.com/)\n",
    "- [CRISP-DM (pdf)](https://www.the-modeling-agency.com/crisp-dm.pdf): CRoss-Industry Standard Process for Data Mining\n",
    "- [ML best practice (pdf)](http://martin.zinkevich.org/rules_of_ml/rules_of_ml.pdf): Best practice for ML Engineering @ Google"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Example: Boston House Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See http://www.neural.cz/dataset-exploration-boston-house-pricing.html\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "# Loading Boston House Prices dataset\n",
    "# This dataset is already cleaned (no missing values)\n",
    "boston_dataset = load_boston()\n",
    "\n",
    "# Print data set Characteristics\n",
    "print(boston_dataset.DESCR)\n",
    "\n",
    "# A dataframe in Pandas is similar to a table\n",
    "df = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)\n",
    "# target here is price (MEDV), but note that we could have chosen another one!\n",
    "df['target'] = boston_dataset.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **model** is an assumption about how the world works. What could be a reasonable model for predicting house price?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(df.describe(percentiles=[]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use = 'default'\n",
    "df.boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas styling the dataframe\n",
    "def highlight_max(s):\n",
    "    '''\n",
    "    highlight the maximum in a Series yellow.\n",
    "    '''\n",
    "    is_max = s == s.max()\n",
    "    return ['background-color: yellow' if v else '' for v in is_max]\n",
    "\n",
    "# Coloring the target column green. For more colors: http://www.w3schools.com/cssref/css_colors.asp\n",
    "df.head(10).style.apply(lambda s: ['background-color: darkseagreen']*s.size, axis=0, subset=['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "scatter_matrix(df, alpha=0.2, figsize=(10, 10), diagonal='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import lag_plot\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "lag_plot(df['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns # conventional alias\n",
    "#sns.set(style=\"white\")\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr = df.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "#cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# By using heatmap twice we get bold all correlations above 0.6\n",
    "sns.heatmap(corr, mask=mask, annot=True)\n",
    "sns.heatmap(corr, mask= abs(corr) < 0.6, cbar=False, annot=True)\n",
    "            #annot_kws={\"weight\": \"bold\"})\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "#sns.heatmap(corr, cmap=cmap, vmax=.3,\n",
    "#            square=True, xticklabels=5, yticklabels=5,\n",
    "#            linewidths=.5, cbar_kws={\"shrink\": .5}, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Want to look further on correlation between RM, LSTAT and house price (MEDV)\n",
    "sns.jointplot(df['RM'], df['target'])\n",
    "sns.jointplot(df['LSTAT'], df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which of the features exhibit a linear correlation with the target?\n",
    "corr.iloc[-1].apply(abs).sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#X = df[['RM', 'LSTAT']]\n",
    "X = df[['LSTAT']]\n",
    "y = df['target']\n",
    "X.boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into (80%) training and (20%) test data\n",
    "X_train, X_test, y_train,  y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Assuming a linear relationship between number of Rooms and House price\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_results(model, X_test, y_test):\n",
    "    # Predict y-values from model:\n",
    "    y_pred = model.predict(X_test)\n",
    "    # The coefficients\n",
    "    #print('Coefficients: \\n', model.coef_)\n",
    "    # The mean squared error\n",
    "    print(\"Mean squared error: %.2f\" % np.mean((y_pred - y_test) ** 2))\n",
    "    # Explained variance score: 1 is perfect prediction\n",
    "    print('Variance score: %.2f' % model.score(X_test, y_test))\n",
    "    # Discarding higher order terms\n",
    "    if X_test.shape[1] > 1:\n",
    "        # Getting second column\n",
    "        X_linear = X_test[:,1]\n",
    "    else:\n",
    "        X_linear = X_test\n",
    "    # Plot outputs\n",
    "    n = y_pred.shape[0]\n",
    "    plt.scatter(X_linear, y_test,  color='black')\n",
    "    X_model = np.linspace(0, X_test.max(), n)\n",
    "    plt.plot(X_model, model.predict(X_model.reshape(-1,1)), color='blue', linewidth=3)\n",
    "\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_results(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear fit does not seem ideal. Let's try quadratic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code based on http://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Input \n",
    "# X: one feature\n",
    "# y: target values\n",
    "# degree: highest order term in polynomial feature\n",
    "def train_poly(X, y, degree=2):\n",
    "    # It is still called linear regression, since it is a linear combination of polynomial terms.\n",
    "    linear_regression = LinearRegression()\n",
    "\n",
    "    polynomial_features = PolynomialFeatures(degree=degree)\n",
    "    pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                         (\"linear_regression\", linear_regression)])\n",
    "    pipeline.fit(X, y)\n",
    "    # Evaluate the models using crossvalidation\n",
    "    scores = cross_val_score(pipeline, X, y,\n",
    "                             scoring=\"neg_mean_squared_error\", cv=10)\n",
    "    print(\"Degree {}\\nMSE = {:.2e}(+/- {:.2e})\".format(\n",
    "        degree, -scores.mean(), scores.std()))\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_poly = train_poly(X_train, y_train)\n",
    "\n",
    "show_results(model_poly, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_results(train_poly(X_train, y_train, degree=5), X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
