{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting into the basics of machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "Why are you here? The short explanation: Because of deep learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is machine learning?\n",
    "https://github.com/amueller/scipy_2015_sklearn_tutorial/blob/master/notebooks/01.1%20Introduction%20to%20Machine%20Learning.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 types of learning problems (by learning style):\n",
    "1. **Supervised learning**: Given X and y find f such that f(X) = y\n",
    "    1. classification (discrete y, a category): E.g. spam/no-spam\n",
    "    2. regression (continuous y, a real value): E.g. house price\n",
    "2. **Unsupervised learning**\n",
    "    1. clustering\n",
    "    2. dimensionality reduction\n",
    "    3. density estimation\n",
    "    4. association: E.g. people that buy A also tend to buy B\n",
    "    5. ...\n",
    "3. **Reinforcement learning** (learning from trial-and-error by rewards/punishment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Check out https://github.com/rasbt/watermark for making reproducible analysis easier.\n",
    "%watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Support for type hints\n",
    "import typing\n",
    "# Based on https://github.com/jakevdp/PMLC-2014/blob/master/fig_code/linear_regression.py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "max_x = 30\n",
    "\n",
    "def generate_linear_data() -> (np.array, np.array):\n",
    "    a = 0.5\n",
    "    b = 1.0\n",
    "\n",
    "    # generate 20 random values in the range [0,max_x)\n",
    "    x = max_x * np.random.random(20)\n",
    "\n",
    "    # y = a*x + b with noise\n",
    "    y = a * x + b + np.random.normal(size=x.shape)\n",
    "    \n",
    "    return x,y\n",
    "\n",
    "    \n",
    "def create_linear_model(x: np.array, y: np.array) -> (np.array, np.array):\n",
    "    # create a linear regression model\n",
    "    ## linear regression attempts to draw a straight line that will \n",
    "    ## best minimize the residual sum of squares between the observed \n",
    "    ## responses in the dataset\n",
    "    model = LinearRegression()\n",
    "    model.fit(x[:, None], y)\n",
    "\n",
    "    # 100 values with equal distance from 0 to max_x\n",
    "    x_pred = np.linspace(0, max_x, 100)\n",
    "    # predict y from the data\n",
    "    y_pred = model.predict(x_pred[:, None])\n",
    "    print(model.get_params())\n",
    "    return x_pred, y_pred\n",
    "\n",
    "\n",
    "def plot_results(x_points: np.array, y_points: np.array,\n",
    "                 x_pred:np.array, y_pred: np.array):\n",
    "    ax = plt.axes()\n",
    "    # Original values\n",
    "    ax.scatter(x_points, y_points)\n",
    "    # Estimated curve\n",
    "    ax.plot(x_pred, y_pred)\n",
    "\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "\n",
    "    ax.axis('tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# * expands the tuple returned from generate_linear_data into values\n",
    "x, y = generate_linear_data()\n",
    "x_pred, y_pred = create_linear_model(x, y)\n",
    "plot_results(x, y, x_pred, y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pylab as pl\n",
    "from sklearn import neighbors, datasets, linear_model\n",
    "\n",
    "f = lambda t: 1.2 * t**2 + .1 * t**3 - .4 * t **5 - .5 * t ** 9\n",
    "\n",
    "def generate_polynomial_data() -> (np.array, np.array):\n",
    "    rng = np.random.RandomState(0)\n",
    "    # Uniformly distribute x in (-1, 1)\n",
    "    x = 2*rng.rand(100) - 1\n",
    "\n",
    "    y = f(x) + .4 * rng.normal(size=100)\n",
    "    \n",
    "    return x,y\n",
    "    \n",
    "def plot_polynomial_regression():\n",
    "    x,y = generate_polynomial_data()\n",
    "    x_test = np.linspace(-1, 1, 100)\n",
    "\n",
    "    pl.figure()\n",
    "    pl.scatter(x, y, s=4)\n",
    "\n",
    "    X = np.array([x**i for i in range(5)]).T\n",
    "    X_test = np.array([x_test**i for i in range(5)]).T\n",
    "    regr = linear_model.LinearRegression()\n",
    "    regr.fit(X, y)\n",
    "    pl.plot(x_test, regr.predict(X_test), label='4th order')\n",
    "\n",
    "    X = np.array([x**i for i in range(10)]).T\n",
    "    X_test = np.array([x_test**i for i in range(10)]).T\n",
    "    regr = linear_model.LinearRegression()\n",
    "    regr.fit(X, y)\n",
    "    pl.plot(x_test, regr.predict(X_test), label='9th order')\n",
    "\n",
    "    pl.legend(loc='best')\n",
    "    pl.axis('tight')\n",
    "    pl.title('Fitting a 4th and a 9th order polynomial')\n",
    "\n",
    "    pl.figure()\n",
    "    pl.scatter(x, y, s=4)\n",
    "    pl.plot(x_test, f(x_test), label=\"truth\")\n",
    "    pl.axis('tight')\n",
    "    pl.title('Ground truth (9th order polynomial)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use this to display overfitting\n",
    "plot_polynomial_regression()\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Create color maps for 3-class classification problem, as with iris\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "def plot_iris_knn():\n",
    "    iris = datasets.load_iris()\n",
    "    X = iris.data[:, :2]  # we only take the first two features. We could\n",
    "                        # avoid this ugly slicing by using a two-dim dataset\n",
    "    y = iris.target\n",
    "\n",
    "    knn = neighbors.KNeighborsClassifier(n_neighbors=3)\n",
    "    knn.fit(X, y)\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - .1, X[:, 0].max() + .1\n",
    "    y_min, y_max = X[:, 1].min() - .1, X[:, 1].max() + .1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                         np.linspace(y_min, y_max, 100))\n",
    "    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    pl.figure()\n",
    "    pl.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "    # Plot also the training points\n",
    "    pl.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)\n",
    "    pl.xlabel('sepal length (cm)')\n",
    "    pl.ylabel('sepal width (cm)')\n",
    "    pl.axis('tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_iris_knn()\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Business/Problem understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is the problem?\n",
    "  - Informal description\n",
    "  - Formal description\n",
    "2. Why does the problem need to be solved?\n",
    "3. How would I solve the problem manually?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Collect initial data\n",
    "Which data sources do we have available?\n",
    "- Internal\n",
    "- Partners\n",
    "- External\n",
    "\n",
    "What format?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Describe data\n",
    "    - What are the features?\n",
    "    - How many samples?\n",
    "3. Explore data\n",
    "4. Verify data quality\n",
    "    - Missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd  # Data Analysis Library: high-performance, easy-to-use data structures and data analysis tools\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load dataset\n",
    "iris_dataset = load_iris()\n",
    "# Describe (provided description in dataset)\n",
    "print(iris_dataset.DESCR)\n",
    "# TODO: Picture of the different flowers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loading data into pandas Dataframe\n",
    "iris_data = pd.DataFrame(iris_dataset.data, columns=iris_dataset.feature_names)\n",
    "print(iris_dataset.target_names)\n",
    "# Convert 0,1,2 in target column to iris_dataset.target_names\n",
    "iris_data['class'] = iris_dataset.target\n",
    "iris_data['class'] = iris_data['class'].apply(lambda i: iris_dataset.target_names[i])\n",
    "iris_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Summary statistics about dataset\n",
    "iris_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: All features are numbers. If not, we have to convert them by e.g. One-hot-encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Display plots inside notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt  # Plotting library\n",
    "import seaborn as sb  # Statistical data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot pairwise relationships in dataset\n",
    "# Note: Only possible to print in 2D (3D), so thats a visualization restriction even when we work on higher dimensional data.\n",
    "sb.pairplot(iris_data, hue='class')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# setosa has an outlier on sepal width\n",
    "print(iris_data.columns)\n",
    "iris_data.loc[iris_data['class'] == 'setosa', 'sepal width (cm)'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "for column_index, column in enumerate(iris_data.columns):\n",
    "    if column == 'class':\n",
    "        continue\n",
    "    plt.subplot(2, 2, column_index + 1)    \n",
    "    # Boxplot\n",
    "    sb.boxplot(x='class', y=column, data=iris_data)\n",
    "    # Violinplot is like boxplot but box is scaled according to density of the data\n",
    "    #sb.violinplot(x='class', y=column, data=iris_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Preparation\n",
    "Really important, but not prioritized in this short introduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Select data\n",
    "2. Clean data\n",
    "    - Drop samples with missing features?\n",
    "3. Construct data\n",
    "    - Impute missing data?\n",
    "4. Integrate data\n",
    "5. Format data\n",
    "6. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bad data = Bad model** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Select modelling technique\n",
    "2. Generate test design\n",
    "3. Build model\n",
    "4. Assess model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data in input (X) and output (y) variables\n",
    "y = iris_data['class'].values\n",
    "X = iris_data.drop('class', axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# split data into training and test set\n",
    "\n",
    "(X_train, X_test, y_train, y_test) = train_test_split(X, y, train_size=0.75, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create the classifier\n",
    "decision_tree_classifier = DecisionTreeClassifier()\n",
    "\n",
    "# Train the classifier on the training set\n",
    "decision_tree_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Validate the classifier on the testing set using classification accuracy\n",
    "decision_tree_classifier.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is that good? Depends on the sampling in train/test-split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check how accuracy varies with different samples (random_state is not fixed)\n",
    "model_accuracies = []\n",
    "\n",
    "for repetition in range(1000):\n",
    "    (X_train, X_test, y_train, y_test) = train_test_split(X, y, train_size=0.75)\n",
    "    \n",
    "    decision_tree_classifier = DecisionTreeClassifier()\n",
    "    decision_tree_classifier.fit(X_train, y_train)\n",
    "    classifier_accuracy = decision_tree_classifier.score(X_test, y_test)\n",
    "    model_accuracies.append(classifier_accuracy)\n",
    "    \n",
    "sb.distplot(model_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Big variations in how our model perform for different subsets of training data.\n",
    "\n",
    "Does not generalize well on new data (X_test) => **overfitting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem is the main reason that most data scientists perform **k-fold cross-validation** on their models: Split the original data set into k subsets, use one of the subsets as the testing set, and the rest of the subsets are used as the training set. This process is then repeated k times such that each subset is used as the testing set exactly once.\n",
    "10-fold cross-validation is the most common choice, so let's use that here. Performing 10-fold cross-validation on our data set looks something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def plot_cv(cv, n_samples):\n",
    "    masks = []\n",
    "    for train, test in cv:\n",
    "        mask = np.zeros(n_samples, dtype=bool)\n",
    "        mask[test] = 1\n",
    "        masks.append(mask)\n",
    "        \n",
    "    plt.figure(figsize=(15, 15))\n",
    "    plt.imshow(masks, interpolation='none')\n",
    "    plt.ylabel('Fold')\n",
    "    plt.xlabel('Row #')\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "plot_cv(skf.split(X,y), len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stratified k-fold cross-validation**: Stratified means the class proportions the same across all of the folds, so we maintain a representative subset of our data set. (e.g., so we don't have 100% Iris setosa entries in one of the folds.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "decision_tree_classifier = DecisionTreeClassifier()\n",
    "\n",
    "# cross_val_score returns a list of the scores, which we can visualize\n",
    "# to get a reasonable estimate of our classifier's performance\n",
    "cv_scores = cross_val_score(decision_tree_classifier, X, y, cv=10)\n",
    "sb.distplot(cv_scores)\n",
    "plt.title('Average score: {}'.format(np.mean(cv_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limiting the depth of decision tree classifier gives much lower accuracy. What hyperparameter choice is best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "decision_tree_classifier = DecisionTreeClassifier(max_depth=1)\n",
    "\n",
    "cv_scores = cross_val_score(decision_tree_classifier, X, y, cv=10)\n",
    "sb.distplot(cv_scores, kde=False)\n",
    "plt.title('Average score: {}'.format(np.mean(cv_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One common systematic way to find best parameters for model and data set: **Grid Search**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "decision_tree_classifier = DecisionTreeClassifier()\n",
    "\n",
    "# Which parameters can be searched\n",
    "print(decision_tree_classifier.get_params())\n",
    "\n",
    "parameter_grid = {'max_depth': [1, 2, 3, 4, 5],\n",
    "                  'max_features': [1, 2, 3, 4]}\n",
    "\n",
    "grid_search = GridSearchCV(decision_tree_classifier,\n",
    "                           param_grid=parameter_grid,\n",
    "                           cv=10)\n",
    "\n",
    "grid_search.fit(X, y)\n",
    "print('Best score: {}'.format(grid_search.best_score_))\n",
    "print('Best parameters: {}'.format(grid_search.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "grid_visualization = []\n",
    "\n",
    "for grid_pair in grid_search.grid_scores_:\n",
    "    grid_visualization.append(grid_pair.mean_validation_score)\n",
    "    \n",
    "grid_visualization = np.array(grid_visualization)\n",
    "grid_visualization.shape = (5, 4)\n",
    "sb.heatmap(grid_visualization, cmap='Blues')\n",
    "plt.xticks(np.arange(4) + 0.5, grid_search.param_grid['max_features'])\n",
    "plt.yticks(np.arange(5) + 0.5, grid_search.param_grid['max_depth'][::-1])\n",
    "plt.xlabel('max_features')\n",
    "plt.ylabel('max_depth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "decision_tree_classifier = DecisionTreeClassifier()\n",
    "\n",
    "parameter_grid = {'criterion': ['gini', 'entropy'],\n",
    "                  'splitter': ['best', 'random'],\n",
    "                  'max_depth': [1, 2, 3, 4, 5],\n",
    "                  'max_features': [1, 2, 3, 4]}\n",
    "\n",
    "grid_search = GridSearchCV(decision_tree_classifier,\n",
    "                           param_grid=parameter_grid,\n",
    "                           cv=10)\n",
    "\n",
    "grid_search.fit(X, y)\n",
    "print('Best score: {}'.format(grid_search.best_score_))\n",
    "print('Best parameters: {}'.format(grid_search.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "decision_tree_classifier = grid_search.best_estimator_\n",
    "decision_tree_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn.tree as tree\n",
    "from sklearn.externals.six import StringIO\n",
    "\n",
    "with open('iris_dtc.dot', 'w') as out_file:\n",
    "    out_file = tree.export_graphviz(decision_tree_classifier, out_file=out_file)\n",
    "# TODO: Display graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "random_forest_classifier = RandomForestClassifier()\n",
    "\n",
    "parameter_grid = {'n_estimators': [5, 10, 25, 50],\n",
    "                  'criterion': ['gini', 'entropy'],\n",
    "                  'max_features': [1, 2, 3, 4],\n",
    "                  'warm_start': [True, False]}\n",
    "\n",
    "grid_search = GridSearchCV(random_forest_classifier,\n",
    "                           param_grid=parameter_grid,\n",
    "                           cv=10)\n",
    "\n",
    "grid_search.fit(X, y)\n",
    "print('Best score: {}'.format(grid_search.best_score_))\n",
    "print('Best parameters: {}'.format(grid_search.best_params_))\n",
    "\n",
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# 5. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Evaluate results\n",
    "2. Review process\n",
    "    - Reproducibility\n",
    "3. Determine next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Deployment (N/A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- [machinelearningmastery.com](http://machinelearningmastery.com/): An excellent blog by Jason Brownlee\n",
    "- [KDnuggets.com](http://www.kdnuggets.com/)\n",
    "- [CRISP-DM](https://www.the-modeling-agency.com/crisp-dm.pdf): CRoss-Industry Standard Process for Data Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Example: Boston House Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# See http://www.neural.cz/dataset-exploration-boston-house-pricing.html\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "# Loading Boston House Prices dataset\n",
    "boston_dataset = load_boston()\n",
    "\n",
    "# Data set Characteristics\n",
    "print(boston_dataset.DESCR)\n",
    "\n",
    "df = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)\n",
    "# target here is price (MEDV), but note that we could have chosen another one!\n",
    "df['target'] = boston_dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(df.describe(percentiles=[]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use = 'default'\n",
    "df.boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pandas styling the dataframe\n",
    "def highlight_max(s):\n",
    "    '''\n",
    "    highlight the maximum in a Series yellow.\n",
    "    '''\n",
    "    is_max = s == s.max()\n",
    "    return ['background-color: yellow' if v else '' for v in is_max]\n",
    "\n",
    "# Coloring the target column green. For more colors: http://www.w3schools.com/cssref/css_colors.asp\n",
    "df.head(10).style.apply(lambda s: ['background-color: darkseagreen']*s.size, axis=0, subset=['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pandas.tools.plotting import scatter_matrix\n",
    "\n",
    "scatter_matrix(df, alpha=0.2, figsize=(10, 10), diagonal='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pandas.tools.plotting import lag_plot\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "lag_plot(df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns # conventional alias\n",
    "#sns.set(style=\"white\")\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr = df.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "#cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# By using heatmap twice we get bold all correlations above 0.6\n",
    "sns.heatmap(corr, mask=mask, annot=True)\n",
    "sns.heatmap(corr, mask= abs(corr) < 0.6, cbar=False, annot=True)\n",
    "            #annot_kws={\"weight\": \"bold\"})\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "#sns.heatmap(corr, cmap=cmap, vmax=.3,\n",
    "#            square=True, xticklabels=5, yticklabels=5,\n",
    "#            linewidths=.5, cbar_kws={\"shrink\": .5}, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Want to look further on correlation between RM, LSTAT and house price (MEDV)\n",
    "sns.jointplot(df['RM'], df['target'])\n",
    "#sns.jointplot(df['LSTAT'], df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Which of the features exhibit a linear correlation with the target?\n",
    "corr.ix[-1].apply(abs).sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = df[['RM', 'LSTAT']]\n",
    "y = df['target']\n",
    "X.boxplot()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
